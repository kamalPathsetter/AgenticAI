{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b9bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "file_path = '/Users/kamal/Desktop/AgenticAI/Uploads/Attention Is All You Need.pdf'\n",
    "\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,           \n",
    "    strategy=\"hi_res\",                     \n",
    "\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],   \n",
    "    image_output_dir_path=\"/Users/kamal/Desktop/AgenticAI/Side Projects/RAG Assignment/Images\",   \n",
    "\n",
    "    extract_image_block_to_payload=True,   \n",
    "\n",
    "    chunking_strategy=\"by_title\",          \n",
    "    max_characters=10000,                  \n",
    "    combine_text_under_n_chars=2000,       \n",
    "    new_after_n_chars=6000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30325938",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc300fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "from collections import Counter\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "image_output_dir = \"/Users/kamal/Desktop/AgenticAI/Side Projects/RAG Assignment/Images\"\n",
    "os.makedirs(image_output_dir, exist_ok=True)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk_id = str(uuid.uuid4())\n",
    "    content_pieces, page_numbers, image_paths = [], [], []\n",
    "\n",
    "    for idx, elem in enumerate(getattr(chunk.metadata, 'orig_elements', [])):\n",
    "        if (text := getattr(elem, 'text', None)):\n",
    "            content_pieces.append(text)\n",
    "\n",
    "        if (page := getattr(elem.metadata, 'page_number', None)) is not None:\n",
    "            page_numbers.append(page)\n",
    "\n",
    "        if (b64 := getattr(elem.metadata, 'image_base64', None)):\n",
    "            try:\n",
    "                img_bytes = base64.b64decode(b64)\n",
    "                img_path = os.path.join(image_output_dir, f\"{chunk_id}_{idx}.png\")\n",
    "                with open(img_path, \"wb\") as f:\n",
    "                    f.write(img_bytes)\n",
    "                image_paths.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Image decode error in chunk {chunk_id}: {e}\")\n",
    "\n",
    "    content = \"\\n\".join(content_pieces)\n",
    "    page_number = Counter(page_numbers).most_common(1)[0][0] if page_numbers else -1\n",
    "\n",
    "    try:\n",
    "        embedding_response = openai.embeddings.create(\n",
    "            input=content,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        embedding = embedding_response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error in chunk {chunk_id}: {e}\")\n",
    "        embedding = [0.0] * 1536\n",
    "\n",
    "    doc = {\n",
    "        \"id\": chunk_id,\n",
    "        \"embedding\": embedding,\n",
    "        \"content\": content,\n",
    "        \"type\": getattr(chunk, 'type', \"Composite\"),\n",
    "        \"section_title\": getattr(chunk, 'section_title', \"\") or \"\",\n",
    "        \"page_number\": page_number,\n",
    "        \"image_path\": image_paths,\n",
    "    }\n",
    "\n",
    "    documents.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, list_collections\n",
    "\n",
    "# 1. Connect to Milvus\n",
    "connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# 2. Define schema\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=36),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=1536),\n",
    "    FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"type\", dtype=DataType.VARCHAR, max_length=20),\n",
    "    FieldSchema(name=\"section_title\", dtype=DataType.VARCHAR, max_length=256),\n",
    "    FieldSchema(name=\"page_number\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"image_path\", dtype=DataType.VARCHAR, max_length=1024),\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields, description=\"RAG Assignment\")\n",
    "collection_name = \"rag_assignment\"\n",
    "\n",
    "# 3. Create collection if not exists\n",
    "if collection_name not in list_collections():\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "else:\n",
    "    collection = Collection(name=collection_name)\n",
    "\n",
    "# 4. Prepare and insert data\n",
    "data_to_insert = [\n",
    "    [doc[\"id\"] for doc in documents],\n",
    "    [doc[\"embedding\"] for doc in documents],\n",
    "    [doc[\"content\"][:4096] for doc in documents],\n",
    "    [doc[\"type\"] for doc in documents],\n",
    "    [doc[\"section_title\"][:256] for doc in documents],\n",
    "    [doc[\"page_number\"] for doc in documents],\n",
    "    [\",\".join(doc[\"image_path\"])[:1024] for doc in documents],\n",
    "]\n",
    "\n",
    "collection.insert(data_to_insert)\n",
    "collection.flush()\n",
    "\n",
    "index_params = {\n",
    "    \"metric_type\": \"COSINE\",\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"params\": {\"nlist\": 128}\n",
    "}\n",
    "\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "\n",
    "collection.load()\n",
    "\n",
    "print(f\"Inserted {len(documents)} documents into Milvus collection '{collection_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2714d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Milvus\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"rag_assignment\",\n",
    "    connection_args={\"host\": \"localhost\", \"port\": \"19530\"},\n",
    "    text_field=\"content\",\n",
    "    vector_field=\"embedding\",     \n",
    "    index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"COSINE\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "query = \"What is the multihead attention?\"\n",
    "top_k_docs = retriever.get_relevant_documents(query)\n",
    "candidate_texts = [doc.page_content for doc in top_k_docs]\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\n",
    "pairs = [(query, text) for text in candidate_texts]\n",
    "scores = cross_encoder.predict(pairs)\n",
    "\n",
    "scored_docs = sorted(zip(top_k_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "ranked_docs = [doc for doc, score in scored_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in ranked_docs[:2]])\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response = chain.run({\"context\": context, \"question\": query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01295372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Load prompt and LLM\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "# Prepare top 2 documents\n",
    "top_docs = ranked_docs[:2]\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in top_docs])\n",
    "\n",
    "# Run RAG synthesis\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response = chain.run({\"context\": context, \"question\": query})\n",
    "\n",
    "# Print response\n",
    "print(\"🔍 Synthesized Answer:\\n\")\n",
    "print(response)\n",
    "\n",
    "# Display associated images (if any)\n",
    "print(\"\\n🖼️ Associated Images from Top Documents:\\n\")\n",
    "for i, doc in enumerate(top_docs, start=1):\n",
    "    image_paths = doc.metadata.get(\"image_path\", \"\")\n",
    "    if image_paths:\n",
    "        paths = [p.strip() for p in image_paths.split(\",\") if p.strip()]\n",
    "        for path in paths:\n",
    "            if os.path.exists(path):\n",
    "                print(f\"Document {i} - Image: {os.path.basename(path)}\")\n",
    "                display(Image(filename=path))\n",
    "            else:\n",
    "                print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39cf6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
