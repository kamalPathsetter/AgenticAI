{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b9bc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b199e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8503b1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamal/miniconda3/envs/agenticaienv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "file_path = '/Users/kamal/Desktop/AgenticAI/Uploads/Attention Is All You Need.pdf'\n",
    "\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,           \n",
    "    strategy=\"hi_res\",                     \n",
    "\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],   \n",
    "    image_output_dir_path=\"/Users/kamal/Desktop/AgenticAI/Side Projects/RAG Assignment/Images\",   \n",
    "\n",
    "    extract_image_block_to_payload=True,   \n",
    "\n",
    "    chunking_strategy=\"by_title\",          \n",
    "    max_characters=10000,                  \n",
    "    combine_text_under_n_chars=2000,       \n",
    "    new_after_n_chars=6000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c7f3628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<unstructured.documents.elements.CompositeElement at 0x17ff072e0>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x17ff07a60>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x17ff07c70>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x17ff078b0>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x17ff07e20>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x17ff07e50>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x17c6b43d0>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x17c6b4340>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x17ff06ce0>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x17ff34d90>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x17c6b40d0>,\n",
       "  <unstructured.documents.elements.CompositeElement at 0x17ff25ea0>],\n",
       " 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks, len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30325938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<unstructured.documents.elements.CompositeElement at 0x17ff06ce0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc300fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "from collections import Counter\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "image_output_dir = \"/Users/kamal/Desktop/AgenticAI/Side Projects/RAG Assignment/Images\"\n",
    "os.makedirs(image_output_dir, exist_ok=True)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk_id = str(uuid.uuid4())\n",
    "    content_pieces, page_numbers, image_paths = [], [], []\n",
    "\n",
    "    for idx, elem in enumerate(getattr(chunk.metadata, 'orig_elements', [])):\n",
    "        if (text := getattr(elem, 'text', None)):\n",
    "            content_pieces.append(text)\n",
    "\n",
    "        if (page := getattr(elem.metadata, 'page_number', None)) is not None:\n",
    "            page_numbers.append(page)\n",
    "\n",
    "        if (b64 := getattr(elem.metadata, 'image_base64', None)):\n",
    "            try:\n",
    "                img_bytes = base64.b64decode(b64)\n",
    "                img_path = os.path.join(image_output_dir, f\"{chunk_id}_{idx}.png\")\n",
    "                with open(img_path, \"wb\") as f:\n",
    "                    f.write(img_bytes)\n",
    "                image_paths.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Image decode error in chunk {chunk_id}: {e}\")\n",
    "\n",
    "    content = \"\\n\".join(content_pieces)\n",
    "    page_number = Counter(page_numbers).most_common(1)[0][0] if page_numbers else -1\n",
    "\n",
    "    try:\n",
    "        embedding_response = openai.embeddings.create(\n",
    "            input=content,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        embedding = embedding_response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error in chunk {chunk_id}: {e}\")\n",
    "        embedding = [0.0] * 1536\n",
    "\n",
    "    doc = {\n",
    "        \"id\": chunk_id,\n",
    "        \"embedding\": embedding,\n",
    "        \"content\": content,\n",
    "        \"type\": getattr(chunk, 'type', \"Composite\"),\n",
    "        \"section_title\": getattr(chunk, 'section_title', \"\") or \"\",\n",
    "        \"page_number\": page_number,\n",
    "        \"image_path\": image_paths,\n",
    "    }\n",
    "\n",
    "    documents.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bfb4bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 12 documents into Milvus collection 'rag_assignment_ipynb'.\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, list_collections\n",
    "\n",
    "# 1. Connect to Milvus\n",
    "connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# 2. Define schema\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=36),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=1536),\n",
    "    FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"type\", dtype=DataType.VARCHAR, max_length=20),\n",
    "    FieldSchema(name=\"section_title\", dtype=DataType.VARCHAR, max_length=256),\n",
    "    FieldSchema(name=\"page_number\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"image_path\", dtype=DataType.VARCHAR, max_length=1024),\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields, description=\"RAG Assignment\")\n",
    "collection_name = \"rag_assignment_ipynb\"\n",
    "\n",
    "# 3. Create collection if not exists\n",
    "if collection_name not in list_collections():\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "else:\n",
    "    collection = Collection(name=collection_name)\n",
    "\n",
    "# 4. Prepare and insert data\n",
    "data_to_insert = [\n",
    "    [doc[\"id\"] for doc in documents],\n",
    "    [doc[\"embedding\"] for doc in documents],\n",
    "    [doc[\"content\"][:4096] for doc in documents],\n",
    "    [doc[\"type\"] for doc in documents],\n",
    "    [doc[\"section_title\"][:256] for doc in documents],\n",
    "    [doc[\"page_number\"] for doc in documents],\n",
    "    [\",\".join(doc[\"image_path\"])[:1024] for doc in documents],\n",
    "]\n",
    "\n",
    "collection.insert(data_to_insert)\n",
    "collection.flush()\n",
    "\n",
    "index_params = {\n",
    "    \"metric_type\": \"COSINE\",\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"params\": {\"nlist\": 128}\n",
    "}\n",
    "\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "\n",
    "collection.load()\n",
    "\n",
    "print(f\"Inserted {len(documents)} documents into Milvus collection '{collection_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2714d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_r/5tgc_pmx6z36cqlgkrdf1k_r0000gr/T/ipykernel_96146/3867490706.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
      "/var/folders/_r/5tgc_pmx6z36cqlgkrdf1k_r0000gr/T/ipykernel_96146/3867490706.py:6: LangChainDeprecationWarning: The class `Milvus` was deprecated in LangChain 0.2.0 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-milvus package and should be used instead. To use it run `pip install -U :class:`~langchain-milvus` and import as `from :class:`~langchain_milvus import MilvusVectorStore``.\n",
      "  vector_store = Milvus(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Milvus\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"rag_assignment\",\n",
    "    connection_args={\"host\": \"localhost\", \"port\": \"19530\"},\n",
    "    text_field=\"content\",\n",
    "    vector_field=\"embedding\",     \n",
    "    index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"COSINE\"},\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91689c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank_bm25 in /Users/kamal/miniconda3/envs/agenticaienv/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /Users/kamal/miniconda3/envs/agenticaienv/lib/python3.10/site-packages (from rank_bm25) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "656b17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "bm25_documents = [\n",
    "    Document(\n",
    "        page_content=doc[\"content\"],\n",
    "        metadata={\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"type\": doc[\"type\"],\n",
    "            \"section_title\": doc[\"section_title\"],\n",
    "            \"page_number\": doc[\"page_number\"],\n",
    "            \"image_path\": doc[\"image_path\"]\n",
    "        }\n",
    "    )\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "from langchain.retrievers import BM25Retriever\n",
    "\n",
    "sparse_retriever = BM25Retriever.from_documents(bm25_documents)\n",
    "sparse_retriever.k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b3ce2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': '42887f8d-8b6e-4d27-b359-ad8c29878179', 'type': 'Composite', 'section_title': '', 'page_number': 3, 'image_path': ['/Users/kamal/Desktop/AgenticAI/Side Projects/RAG Assignment/Images/42887f8d-8b6e-4d27-b359-ad8c29878179_3.png']}, page_content='3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1,...,xn) to a sequence of continuous representations z = (z1,...,zn). Given z, the decoder then generates an output sequence (y1,...,ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nOutput Probabilities Add & Norm Feed Forward Add & Norm Multi-Head Attention a, Add & Norm Add & Norm Feed Forward Nx | Cag Norm) Add & Norm Masked Multi-Head Multi-Head Attention Attention Se a, Lt Positional Positional Encoding @ © OY Encoding Input Output Embedding Embedding Inputs Outputs (shifted right)\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.'),\n",
       " Document(metadata={'id': '9e459a01-1c9e-442f-912c-e5e695651ae6', 'type': 'Composite', 'section_title': '', 'page_number': 4, 'image_path': ['/Users/kamal/Desktop/AgenticAI/Side Projects/RAG Assignment/Images/9e459a01-1c9e-442f-912c-e5e695651ae6_5.png', '/Users/kamal/Desktop/AgenticAI/Side Projects/RAG Assignment/Images/9e459a01-1c9e-442f-912c-e5e695651ae6_6.png']}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nLinear\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the √ dk, and apply a softmax function to obtain the weights on the query with all keys, divide each by values.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\\nAttention(Q,K,V ) = softmax( QKT √ dk )V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor 1√ of . Additive attention computes the compatibility function using a feed-forward network with dk a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ . dk'),\n",
       " Document(metadata={'id': 'b903c119-9959-44af-b00b-e9797fde369c', 'type': 'Composite', 'section_title': '', 'page_number': 5, 'image_path': []}, page_content='3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n‘To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, g -k = ves, qiki, has mean 0 and variance dx.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q,K,V ) = Concat(head1,...,headh)W O where headi = Attention(QW Q i ,KW K i ,V W V i )\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel. i ∈ Rdmodel×dk, W K i ∈ Rdmodel×dk, W V i ∈ Rdmodel×dv\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_retriever.invoke(\"Multi-Head Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c2afa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[sparse_retriever, retriever],  # [sparse, dense]\n",
    "    weights=[0.7, 0.3]  # alpha = 0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce89ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Multi-Head Attention\"\n",
    "top_k_docs = ensemble_retriever.invoke(query)\n",
    "\n",
    "# Use your reranking pipeline\n",
    "from sentence_transformers import CrossEncoder\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\n",
    "\n",
    "pairs = [(query, doc.page_content) for doc in top_k_docs]\n",
    "scores = cross_encoder.predict(pairs)\n",
    "\n",
    "scored_docs = sorted(zip(top_k_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "ranked_docs = [doc for doc, score in scored_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac8e0d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': '9e459a01-1c9e-442f-912c-e5e695651ae6', 'type': 'Composite', 'section_title': '', 'page_number': 4, 'image_path': ['/Users/kamal/Desktop/AgenticAI/Side Projects/RAG Assignment/Images/9e459a01-1c9e-442f-912c-e5e695651ae6_5.png', '/Users/kamal/Desktop/AgenticAI/Side Projects/RAG Assignment/Images/9e459a01-1c9e-442f-912c-e5e695651ae6_6.png']}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nLinear\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the √ dk, and apply a softmax function to obtain the weights on the query with all keys, divide each by values.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\\nAttention(Q,K,V ) = softmax( QKT √ dk )V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor 1√ of . Additive attention computes the compatibility function using a feed-forward network with dk a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ . dk'),\n",
       " Document(metadata={'id': 'b903c119-9959-44af-b00b-e9797fde369c', 'type': 'Composite', 'section_title': '', 'page_number': 5, 'image_path': []}, page_content='3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n‘To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, g -k = ves, qiki, has mean 0 and variance dx.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q,K,V ) = Concat(head1,...,headh)W O where headi = Attention(QW Q i ,KW K i ,V W V i )\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel. i ∈ Rdmodel×dk, W K i ∈ Rdmodel×dk, W V i ∈ Rdmodel×dv\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.'),\n",
       " Document(metadata={'id': '42887f8d-8b6e-4d27-b359-ad8c29878179', 'type': 'Composite', 'section_title': '', 'page_number': 3, 'image_path': ['/Users/kamal/Desktop/AgenticAI/Side Projects/RAG Assignment/Images/42887f8d-8b6e-4d27-b359-ad8c29878179_3.png']}, page_content='3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1,...,xn) to a sequence of continuous representations z = (z1,...,zn). Given z, the decoder then generates an output sequence (y1,...,ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nOutput Probabilities Add & Norm Feed Forward Add & Norm Multi-Head Attention a, Add & Norm Add & Norm Feed Forward Nx | Cag Norm) Add & Norm Masked Multi-Head Multi-Head Attention Attention Se a, Lt Positional Positional Encoding @ © OY Encoding Input Output Embedding Embedding Inputs Outputs (shifted right)\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.'),\n",
       " Document(metadata={'id': '2fe192df-dff7-4e5b-8236-f6a8faeb1317', 'type': 'Composite', 'section_title': '', 'page_number': 12, 'image_path': 'uploaded_images/Attention Is All You Need.pdf_2fe192df-dff7-4e5b-8236-f6a8faeb1317_7.png,uploaded_images/Attention Is All You Need.pdf_2fe192df-dff7-4e5b-8236-f6a8faeb1317_10.png,uploaded_images/Attention Is All You Need.pdf_2fe192df-dff7-4e5b-8236-f6a8faeb1317_13.png,uploaded_images/Attention Is All You Need.pdf_2fe192df-dff7-4e5b-8236-f6a8faeb1317_14.png', 'pdf_name': 'Attention Is All You Need.pdf'}, page_content='[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\n2 c i< gE 3 > w n £. Ke} [a Q €oe2s ozesyuTes 2ea8 T_ FFSREH8TZESHBOP_,SSSE DSsSSSES ~2£F$€voFEnvFESCRCoecacKRGNESLSESSCEC -vVvVVVVV Hv over Boepgqecxuos QKXRDLXE EON “A AAAAAA SPesewe Fog ese ss ss ess 5 PB HSBsgaas Sa 5 SESEHSRBESDS B=] 2 8 oes aaa oO 2 = on aN G o ° ao0000 0 © OE @ £ > 9 wUvvvvVvV Vv — €E& © —e 2 6 v DoD <8 8 & ||\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\n<ped> <ped> <SOa> — 70” UOIUIdO == = uoluldo Aw — Aw ul ul Bulssiw Bulssiw ale « ae aM = aM yeum = yeum S| v2 S| sy si ysnf ysnf 3q° 3q Pinoys Pinoys uojeojdde Ss}! nq jopied 3q JO@AoU me) aul <ped> <SOa> uojuido Aw ul Bulssiw oe aM yeum S| su} ysnf 3q Pinoys uojeoydde si! ynq yooped 3q JOABU meq au <ped> <SOa> uoluldo Aw ul Bulssiw oe OM yeum S| Siu} ysnf 3q Pinoys uoyeodde si! ynq yooped 3q aul\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n14\\n<ped> <ped> <SOS>\\\\ <SO4> uoluido = uoluido Aw Aw yeum S| sin ysn[— w/a -ysn{ _—8q Pinoys“ pinoys uoieadde * uojeodde Ss}! S$}! | ya | ngs inq pooped pooped aq aq Janou™ JO@ABU HIM “TIM me) me) aul “OU\\n<ped> <ped> so ————_<S0 UO|UIdO uoluldo Aw Aw ul ul Bulssiw Bulssiw ae ale aM am yeum yeum s| s| sty} # sly -— - a 3q 3q Pinoys « pinoys uoyjeoijdde ee Ss}! Ss}! inq ee popod eae 3q 3q JOABU JOABU IW IW rn aul aul\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01295372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Synthesized Answer:\n",
      "\n",
      "Multi-Head Attention enhances the attention mechanism by projecting queries, keys, and values into multiple subspaces and performing attention in parallel across these projections. This allows the model to jointly attend to different representation subspaces, improving its ability to capture diverse information. In practice, it consists of several attention heads that are concatenated and projected to produce the final output.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Load prompt and LLM\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "# Prepare top 2 documents\n",
    "top_docs = ranked_docs[:2]\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in top_docs])\n",
    "\n",
    "# Run RAG synthesis\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response = chain.run({\"context\": context, \"question\": query})\n",
    "\n",
    "# Print response\n",
    "print(\"🔍 Synthesized Answer:\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a39cf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nLinear\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the √ dk, and apply a softmax function to obtain the weights on the query with all keys, divide each by values.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\\nAttention(Q,K,V ) = softmax( QKT √ dk )V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor 1√ of . Additive attention computes the compatibility function using a feed-forward network with dk a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ . dk\\n\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n‘To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, g -k = ves, qiki, has mean 0 and variance dx.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q,K,V ) = Concat(head1,...,headh)W O where headi = Attention(QW Q i ,KW K i ,V W V i )\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel. i ∈ Rdmodel×dk, W K i ∈ Rdmodel×dk, W V i ∈ Rdmodel×dv\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd152509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
