{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f637e6c8",
   "metadata": {},
   "source": [
    "# Run Milvus DB from Docker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c8faa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece2178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b1e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = \"/Users/kamal/Desktop/AgenticAI/Uploads\"\n",
    "text_output_dir = \"output/text\"\n",
    "image_output_dir = \"output/images\"\n",
    "table_output_dir = \"output/tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb29f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(text_output_dir, exist_ok=True)\n",
    "os.makedirs(image_output_dir, exist_ok=True)\n",
    "os.makedirs(table_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e72d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ad63e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "len(embedding_model.embed_query(\"hello AI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c0b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66ecff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_pdf(pdf_path, base_filename):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            pix = fitz.Pixmap(doc, xref)\n",
    "            if pix.n < 5:\n",
    "                pix.save(f\"{image_output_dir}/{base_filename}_p{page_num+1}_img{img_index+1}.png\")\n",
    "            else:\n",
    "                pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                pix.save(f\"{image_output_dir}/{base_filename}_p{page_num+1}_img{img_index+1}.png\")\n",
    "            pix = None\n",
    "    doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db57d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_from_pdf(pdf_path):\n",
    "    table_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            for table in page.extract_tables():\n",
    "                df = pd.DataFrame(table)\n",
    "                table_text += df.to_string(index=False) + \"\\n\\n\"\n",
    "    return table_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b5941fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 12:47:34,060 [DEBUG][_create_connection]: Created new connection using: fccd45bb651b4da882c6f9b34be68d6c (async_milvus_client.py:599)\n"
     ]
    }
   ],
   "source": [
    "from langchain_milvus import Milvus\n",
    "from pymilvus import connections, Collection\n",
    "import time\n",
    "\n",
    "URI = \"tcp://localhost:19530\"\n",
    "\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embedding_model,\n",
    "    connection_args={\"uri\": URI},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780819c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_info = {\n",
    "    \"my_collection_flat\": {\n",
    "        \"index_type\": \"FLAT\",\n",
    "        \"params\": {},\n",
    "        \"search_param\": {\"metric_type\": \"COSINE\", \"params\": {}}\n",
    "    },\n",
    "    \"my_collection_hnsw\": {\n",
    "        \"index_type\": \"HNSW\",\n",
    "        \"params\": {\"M\": 8, \"efConstruction\": 64},\n",
    "        \"search_param\": {\"metric_type\": \"COSINE\", \"params\": {\"ef\": 64}}\n",
    "    },\n",
    "    \"my_collection_ivf\": {\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"params\": {\"nlist\": 128},\n",
    "        \"search_param\": {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a36a59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_collection_flat index FLAT created.\n",
      "my_collection_hnsw index HNSW created.\n",
      "my_collection_ivf index IVF_FLAT created.\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import Collection, FieldSchema, CollectionSchema, DataType, utility\n",
    "\n",
    "def create_collection(name):\n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=64),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=1536),\n",
    "        FieldSchema(name=\"metadata\", dtype=DataType.JSON)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, description=\"PDF chunks embeddings\")\n",
    "    collection = Collection(name, schema)\n",
    "    return collection\n",
    "\n",
    "def create_index(collection, index_type: str, params: dict):\n",
    "    collection.release()\n",
    "    collection.drop_index()\n",
    "    collection.create_index(\n",
    "        field_name=\"embedding\",\n",
    "        index_params={\n",
    "            \"index_type\": index_type,\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"params\": params\n",
    "        }\n",
    "    )\n",
    "    collection.load()\n",
    "    print(f\"{collection.name} index {index_type} created.\")\n",
    "\n",
    "collections = {}\n",
    "existing_collections = utility.list_collections()\n",
    "\n",
    "for name, info in collections_info.items():\n",
    "    if name in existing_collections:\n",
    "        collections[name] = Collection(name)\n",
    "    else:\n",
    "        collections[name] = create_collection(name)\n",
    "    create_index(collections[name], info[\"index_type\"], info[\"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "335213a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Processing: llama2.pdf\n",
      "✅ llama2.pdf -> 289 chunks embedded and stored in all collections.\n",
      "🔹 First chunk: Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Tou...\n",
      "📄 Processing: VVKR - EPC Contract.pdf\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m ids \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m collection \u001b[38;5;129;01min\u001b[39;00m collections\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_text\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# add the actual chunk text here!\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     collection\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     45\u001b[0m     collection\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[0;32m~/miniconda3/envs/agenticaienv/lib/python3.10/site-packages/pymilvus/orm/collection.py:516\u001b[0m, in \u001b[0;36mCollection.insert\u001b[0;34m(self, data, partition_name, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39minsert_rows(\n\u001b[1;32m    507\u001b[0m         collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name,\n\u001b[1;32m    508\u001b[0m         entities\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    513\u001b[0m     )\n\u001b[1;32m    515\u001b[0m check_insert_schema(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema, data)\n\u001b[0;32m--> 516\u001b[0m entities \u001b[38;5;241m=\u001b[39m \u001b[43mPrepare\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mbatch_insert(\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name,\n\u001b[1;32m    519\u001b[0m     entities,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/agenticaienv/lib/python3.10/site-packages/pymilvus/orm/prepare.py:98\u001b[0m, in \u001b[0;36mPrepare.prepare_data\u001b[0;34m(cls, data, schema, is_insert)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ParamError(\n\u001b[1;32m     92\u001b[0m             message\u001b[38;5;241m=\u001b[39mwrong_ndarr_type\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m                 field\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.float32/np.float64\u001b[39m\u001b[38;5;124m\"\u001b[39m, f_data\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     94\u001b[0m             )\n\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     96\u001b[0m     d \u001b[38;5;241m=\u001b[39m f_data\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mf_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ndarr \u001b[38;5;129;01min\u001b[39;00m f_data:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_valid_ndarray(ndarr):\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        print(f\"📄 Processing: {filename}\")\n",
    "\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        # tables = extract_tables_from_pdf(pdf_path)\n",
    "        full_text = text\n",
    "\n",
    "        with open(f\"{text_output_dir}/{base_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(full_text)\n",
    "\n",
    "        extract_images_from_pdf(pdf_path, base_name)\n",
    "\n",
    "        chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "        docs = [\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"file_name\": filename, \"chunk_id\": f\"{base_name}_{i}\"}\n",
    "            )\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "\n",
    "        texts = [doc.page_content for doc in docs]\n",
    "        embeddings = embedding_model.embed_documents(texts)\n",
    "        ids = [doc.metadata[\"chunk_id\"] for doc in docs]\n",
    "\n",
    "        for collection in collections.values():\n",
    "            collection.insert([\n",
    "            ids,\n",
    "            embeddings,\n",
    "            [\n",
    "                {\n",
    "                    \"file_name\": filename,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": chunk_text  # add the actual chunk text here!\n",
    "                }\n",
    "                for chunk_id, chunk_text in zip(ids, texts)\n",
    "            ]\n",
    "        ])\n",
    "\n",
    "            collection.flush()\n",
    "            collection.load()\n",
    "\n",
    "        print(f\"✅ {filename} -> {len(chunks)} chunks embedded and stored in all collections.\")\n",
    "        print(f\"🔹 First chunk: {chunks[0][:60]}...\")\n",
    "\n",
    "print(\"🚀 Extraction and embedding complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0469ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load collections\n",
    "flat_col = Collection(\"my_collection_flat\")\n",
    "hnsw_col = Collection(\"my_collection_hnsw\")\n",
    "ivf_col = Collection(\"my_collection_ivf\")\n",
    "\n",
    "# Load into memory\n",
    "flat_col.load()\n",
    "hnsw_col.load()\n",
    "ivf_col.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_collection(collection, query_vector, top_k=5):\n",
    "    search_params = {\n",
    "        \"my_collection_flat\": {\"metric_type\": \"COSINE\", \"params\": {}},\n",
    "        \"my_collection_hnsw\": {\"metric_type\": \"COSINE\", \"params\": {\"ef\": 64}},\n",
    "        \"my_collection_ivf\": {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    results = collection.search(\n",
    "        data=[query_vector],\n",
    "        anns_field=\"embedding\",        \n",
    "        param=search_params[collection.name],\n",
    "        limit=top_k,\n",
    "        output_fields=[\"metadata\"]      \n",
    "    )\n",
    "    end = time.time()\n",
    "    \n",
    "    return results[0], end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33574e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAT time: 0.0108s\n",
      "HNSW time: 0.0034s\n",
      "IVF time: 0.0031s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "query_vector = np.random.rand(1536).tolist()\n",
    "\n",
    "# Search all 3 collections\n",
    "flat_results, flat_time = search_collection(flat_col, query_vector)\n",
    "hnsw_results, hnsw_time = search_collection(hnsw_col, query_vector)\n",
    "ivf_results, ivf_time = search_collection(ivf_col, query_vector)\n",
    "\n",
    "# Show results\n",
    "print(f\"FLAT time: {flat_time:.4f}s\")\n",
    "print(f\"HNSW time: {hnsw_time:.4f}s\")\n",
    "print(f\"IVF time: {ivf_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa29391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymilvus import Collection\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def search_with_cosine_similarity(collection: Collection, query_vector, top_k=5):\n",
    "    \"\"\"Search a Milvus collection and print cosine similarity scores.\"\"\"\n",
    "    \n",
    "    # Set search parameters based on collection name\n",
    "    search_params_map = {\n",
    "        \"my_collection_flat\": {\"metric_type\": \"COSINE\", \"params\": {}},\n",
    "        \"my_collection_hnsw\": {\"metric_type\": \"COSINE\", \"params\": {\"ef\": 64}},\n",
    "        \"my_collection_ivf\":  {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "    }\n",
    "    \n",
    "    search_params = search_params_map.get(collection.name, {\"metric_type\": \"COSINE\", \"params\": {}})\n",
    "    \n",
    "    # Run the vector search\n",
    "    results = collection.search(\n",
    "        data=[query_vector],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=top_k,\n",
    "        output_fields=[\"metadata\", \"embedding\"]\n",
    "    )\n",
    "    \n",
    "    # Process results\n",
    "    print(f\"\\n📦 Collection: {collection.name}\")\n",
    "    for i, hit in enumerate(results[0]):\n",
    "        metadata = hit.entity.get(\"metadata\", \"\")\n",
    "        embedding = hit.entity.get(\"embedding\", None)\n",
    "        \n",
    "        if embedding:\n",
    "            cos_sim = cosine_similarity(query_vector, embedding)\n",
    "            print(f\"Result {i+1}:\")\n",
    "            print(f\" - Metadata (preview): {str(metadata)[:80]}...\")\n",
    "            print(f\" - Cosine Similarity: {cos_sim:.4f}\")\n",
    "        else:\n",
    "            print(f\" - ⚠️ No embedding returned. You may need to fetch it manually by ID.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80256c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful assistant. Use the following context from a PDF to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in clear, elaborated and concise language. If the answer is not in the context, say you don't know.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c904e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_from_hits(hits, top_k=5):\n",
    "    # Extract top-k metadata as context text\n",
    "    return \"\\n\\n\".join([str(hit.entity.get(\"metadata\", \"\")) for hit in hits[:top_k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9889b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Build the chain\n",
    "qa_chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb029da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔮 GPT-4o-mini Answer:\n",
      " Llama 2 comes in several variants with different parameter sizes: 7 billion (7B), 13 billion (13B), and 70 billion (70B) parameters. Additionally, there are 34 billion (34B) variants that were trained but are not being released. Thus, the models range from 7B to 70B parameters.\n"
     ]
    }
   ],
   "source": [
    "question = \"How many parameters are present in Llama?\"\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "query_vector = embedding_model.embed_query(question)\n",
    "\n",
    "hits, _ = search_collection(ivf_col, query_vector)\n",
    "context = build_context_from_hits(hits, top_k=5)\n",
    "\n",
    "# Run the chain\n",
    "response = qa_chain.invoke({\"context\": context, \"question\": question})\n",
    "print(\"\\n🔮 GPT-4o-mini Answer:\\n\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36a124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Collection: my_collection_flat\n",
      "Result 1:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_10', 'text': 'adopted grouped-qu...\n",
      " - Cosine Similarity: 0.6361\n",
      "Result 2:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_288', 'text': 'applications of L...\n",
      " - Cosine Similarity: 0.6204\n",
      "Result 3:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_14', 'text': 'and grouped-query ...\n",
      " - Cosine Similarity: 0.6086\n",
      "Result 4:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_7', 'text': '(such as BLOOM (Sca...\n",
      " - Cosine Similarity: 0.5845\n",
      "Result 5:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_1', 'text': 'Angela Fan Melanie ...\n",
      " - Cosine Similarity: 0.5825\n",
      "\n",
      "📦 Collection: my_collection_hnsw\n",
      "Result 1:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_10', 'text': 'adopted grouped-qu...\n",
      " - Cosine Similarity: 0.6361\n",
      "Result 2:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_288', 'text': 'applications of L...\n",
      " - Cosine Similarity: 0.6204\n",
      "Result 3:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_14', 'text': 'and grouped-query ...\n",
      " - Cosine Similarity: 0.6086\n",
      "Result 4:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_7', 'text': '(such as BLOOM (Sca...\n",
      " - Cosine Similarity: 0.5845\n",
      "Result 5:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_1', 'text': 'Angela Fan Melanie ...\n",
      " - Cosine Similarity: 0.5825\n",
      "\n",
      "📦 Collection: my_collection_ivf\n",
      "Result 1:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_10', 'text': 'adopted grouped-qu...\n",
      " - Cosine Similarity: 0.6361\n",
      "Result 2:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_288', 'text': 'applications of L...\n",
      " - Cosine Similarity: 0.6204\n",
      "Result 3:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_14', 'text': 'and grouped-query ...\n",
      " - Cosine Similarity: 0.6086\n",
      "Result 4:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_7', 'text': '(such as BLOOM (Sca...\n",
      " - Cosine Similarity: 0.5845\n",
      "Result 5:\n",
      " - Metadata (preview): {'file_name': 'llama2.pdf', 'chunk_id': 'llama2_1', 'text': 'Angela Fan Melanie ...\n",
      " - Cosine Similarity: 0.5825\n"
     ]
    }
   ],
   "source": [
    "search_with_cosine_similarity(flat_col, query_vector)\n",
    "search_with_cosine_similarity(hnsw_col, query_vector)\n",
    "search_with_cosine_similarity(ivf_col, query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention is a mechanism in neural networks that allows the model to focus on specific parts of the input sequence when generating an output. It maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values, with weights determined by a compatibility function comparing the query with the corresponding keys. In the \"Scaled Dot-Product Attention\" approach, the dot products of the query with all keys are computed, scaled by the dimension of the keys, and passed through a softmax function to obtain the weights on the values. This mechanism enables the model to selectively prioritize relevant information, aiding in tasks like translation, summarization, and more.\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention is a mechanism in neural networks that allows the model to focus on specific parts of the input sequence when generating an output. It maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values, with weights determined by a compatibility function comparing the query with the corresponding keys. In the \\\"Scaled Dot-Product Attention\\\" approach, the dot products of the query with all keys are computed, scaled by the dimension of the keys, and passed through a softmax function to obtain the weights on the values. This mechanism enables the model to selectively prioritize relevant information, aiding in tasks like translation, summarization, and more.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d33d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multihead Attention is a mechanism that consists of several attention layers running in parallel. It allows the model to attend to different parts of the input sequence differently, capturing various relationships and dependencies within the data. In this approach, multiple sets of queries, keys, and values are created, enabling the model to gather information from multiple perspectives at once, enhancing its ability to understand complex patterns in the input. The outputs from these multiple attention heads are then combined and processed to produce the final result.\n"
     ]
    }
   ],
   "source": [
    "print(\"Multihead Attention is a mechanism that consists of several attention layers running in parallel. It allows the model to attend to different parts of the input sequence differently, capturing various relationships and dependencies within the data. In this approach, multiple sets of queries, keys, and values are created, enabling the model to gather information from multiple perspectives at once, enhancing its ability to understand complex patterns in the input. The outputs from these multiple attention heads are then combined and processed to produce the final result.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
